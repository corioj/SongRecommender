{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Python Notebook to explore different recommender systems, with recommendation of songs based on Spotify data.\n# Since we don't have users' data, we will focus on Content-Based Filtering\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O\nimport sklearn as skl # ML algorithms and processing\nfrom sklearn import metrics\nimport json\nimport math\nimport random\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-27T20:35:58.854862Z","iopub.execute_input":"2021-08-27T20:35:58.855219Z","iopub.status.idle":"2021-08-27T20:35:59.02701Z","shell.execute_reply.started":"2021-08-27T20:35:58.85519Z","shell.execute_reply":"2021-08-27T20:35:59.025893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Read\ntracks = pd.read_csv('/kaggle/input/spotify-dataset-19212020-160k-tracks/tracks.csv')\n\n#artists = pd.read_csv('/kaggle/input/spotify-dataset-19212020-160k-tracks/artists.csv')\n\n#genres = pd.read_csv('/kaggle/input/spotify-dataset-19212020-160k-tracks/data_by_genres_o.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:36:01.518769Z","iopub.execute_input":"2021-08-27T20:36:01.519118Z","iopub.status.idle":"2021-08-27T20:36:06.483923Z","shell.execute_reply.started":"2021-08-27T20:36:01.51909Z","shell.execute_reply":"2021-08-27T20:36:06.482953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NULL CHECKING AND BASIC INVESTIGATION**\n\nWe will now do introductory investigation into each of the files we have read in, mainly just looking for data anomalies or things to clean. More exploratory will come later in the notebook.","metadata":{}},{"cell_type":"code","source":"# Tracks investigation\ntracks.info()\ntracks.describe()\n\n# drop NULL\ntracks.dropna(inplace = True)\nlen(tracks)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:36:09.548496Z","iopub.execute_input":"2021-08-27T20:36:09.548862Z","iopub.status.idle":"2021-08-27T20:36:10.664564Z","shell.execute_reply.started":"2021-08-27T20:36:09.548834Z","shell.execute_reply":"2021-08-27T20:36:10.663137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the investigative code in the above cell, it seems only the \"name\" column has any NULL values, and very few at that. For presentability purposes, these rows will be dropped. None of the other columns have NULL values, and they are the ones on which most of our system will be built around, so dropping the NULL rows should have very little effect.","metadata":{}},{"cell_type":"markdown","source":"Off of initial thoughts, a fair bit of this will be useful in recommmending tracks. Things like the loudness of a track and the danceability for example play a role in determining whether or not I personally will enjoy a track, so these will be important should we decide to recommend.","metadata":{}},{"cell_type":"markdown","source":"**DATA EXPLORATORY**","metadata":{}},{"cell_type":"code","source":"# tracks\n\n# heatmap\nax = sns.heatmap(tracks[['acousticness', 'danceability', 'duration_ms', 'energy',\n                         'instrumentalness', 'liveness', 'loudness', 'speechiness',\n                         'tempo', 'valence', 'popularity']].corr())","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:36:14.014647Z","iopub.execute_input":"2021-08-27T20:36:14.015018Z","iopub.status.idle":"2021-08-27T20:36:14.693206Z","shell.execute_reply.started":"2021-08-27T20:36:14.014989Z","shell.execute_reply":"2021-08-27T20:36:14.692286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above is a heatmap expressing the correlation between various numeric columns in our track dataset. The majority of the correlation values between columns hovers around -0.2 to 0.2. \n\nSome of the notable characteristics are, thus, those with a larger magnitude of correlation, whether negative or positive. Among these, we see a track's acoustiness is negatively correlated with its loudness, energy, and, interestingly, its popularity. A song's energy is very positively correlated with its loudness, tempo, valence, and popularity. From this, we can determine a song with a lot of energy and loudness is more likely to be popular, while lower tempo, more acoustic songs tend to be less popular.\n\nThese sorts of trends may hint at cluster analysis being useful to identify different \"types\" of tracks, possibly with the goal of attributing characteristics and genres to the clusters.","metadata":{}},{"cell_type":"code","source":"# create figure for a 3x3 grid to analyze histograms\nfig, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9)) = plt.subplots(3, 3, sharey = True, figsize=(10,10))\n#fig = plt.figure(figsize=(1, 1))\n\naxes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\nfeatures = ['acousticness', 'danceability', 'duration_ms', 'energy', \n            'instrumentalness', 'liveness', 'loudness', 'speechiness', 'popularity']\n\nfor ax,feat in zip(axes, features):\n    ax.hist(x = tracks[feat], bins = 40)\n    ax.set(xlabel=feat, ylabel='# of tracks')\n\n# examine popularity\n#plt.hist(x = tracks['popularity'], bins = 50)\n#plt.xlabel('Popularity')\n#plt.ylabel('# of Tracks')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:36:18.756345Z","iopub.execute_input":"2021-08-27T20:36:18.756733Z","iopub.status.idle":"2021-08-27T20:36:20.75968Z","shell.execute_reply.started":"2021-08-27T20:36:18.756698Z","shell.execute_reply":"2021-08-27T20:36:20.758445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When a number of the relevant features are analyzed, it appears that the distributions of the traits across all tracks are rarely normal. That is not necessarily surprising, given tracks are typically of a certain length, and generally should be a certain level of loudness if the artist actually wants the listener to hear the music.\n\nSome more interesting things we can see are the approximate normality of the energy feature, and given its comparatively high correlation with a number of features it follows that those as well should be approximately normal. Yet, popularity doesn't follow precisely. Another is how left-skewed a ton of these features are. The liveness, instrumentalness, and speechiness features are all noticeably skewed to values of low magnitude.\n\nThere appears to be a minor left-skew to track popularity as well, meaning a ton of tracks are not very popular at all, with a bit higher density appearing in the range of 20-40. Granted, this makes sense, but it could have a noticeable effect if, moving forward, the goal of our system is to find esoteric tracks or deep-cuts. A slight shift in weight to make popularity more or less important in deciding the track recommended could play a very noticeable role in producing different track recommendations.\n","metadata":{}},{"cell_type":"markdown","source":"**CLUSTER ANALYSIS**","metadata":{}},{"cell_type":"code","source":"# maybe some dimension reduction could help, PCA?\n\nfrom sklearn import cluster as cs\n\n# random data selection\nsubset = tracks[['acousticness', 'danceability', 'duration_ms', 'energy',\n                         'instrumentalness', 'liveness', 'loudness', 'speechiness',\n                         'tempo', 'valence', 'popularity']]\n# take a (hopefully) representative sample of the dataset, since clustering with 600k points is a bit long\nclusterdata = subset.sample(n = 100000)\n\n# evaluation vectors\ninertia = []\nsilhouettescores = []\n# would like to test more, but the time committment for training and evaluating is not worth it\nclusters = [2,4,6,8]\n\n# K-Means & diagnostics\nfor c in clusters:\n    # create our object and fit\n    kmeans_cluster = cs.MiniBatchKMeans(n_clusters = c, batch_size = 100) # batch size = 100 is default\n    kmeans_cluster.fit(clusterdata)\n    # evaluate based on inertia and silhoutte score\n    inertia.append(kmeans_cluster.inertia_)\n    silhouettescores.append(metrics.silhouette_score(clusterdata, kmeans_cluster.labels_, metric = 'euclidean'))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:36:25.349118Z","iopub.execute_input":"2021-08-27T20:36:25.349534Z","iopub.status.idle":"2021-08-27T20:47:48.123537Z","shell.execute_reply.started":"2021-08-27T20:36:25.3495Z","shell.execute_reply":"2021-08-27T20:47:48.122127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the results\n# inertia plot\nplt.plot(clusters, inertia)\nplt.xlabel(\"Cluster #\")\nplt.ylabel(\"Inertia\")\nplt.show()\n\n# silhouette score plot\nplt.plot(clusters, silhouettescores)\nplt.xlabel(\"Cluster #\")\nplt.ylabel(\"Silhouette Scores\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T20:54:38.910051Z","iopub.execute_input":"2021-08-27T20:54:38.910638Z","iopub.status.idle":"2021-08-27T20:54:39.248234Z","shell.execute_reply.started":"2021-08-27T20:54:38.910581Z","shell.execute_reply":"2021-08-27T20:54:39.247201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose a cluster value of 8 in this run, as we can see the inertia minimize at 8, as well as an elbow point after 4 clusters in the silhouette score. This indicates that the formation quality (relative density and separation) of these clusters takes a notable hit when moving from 2 to 4 and above, while inertia steadily improves.\n\nOf course, something to consider carefully is that the cluster algorithm is ran on a random sample of the data about 1/6 of the total size. It follows that the variability of the cluster assignments and the clusters themselves could be producing results different from the optimal clustering, and through multiple runs results have changed. 8 clusters tends to minimize the inertia, while the silhouette score tends to dip at 4 and increase a small amount from then on.","metadata":{}},{"cell_type":"code","source":"# visualizing our clusters, cluster_number = 4\nkmeans_final = cs.MiniBatchKMeans(n_clusters = 8, batch_size = 100).fit(clusterdata)\nclusterdata['labels'] = kmeans_final.labels_\n\n# examining in 2D\n# 'acousticness', 'danceability', 'duration_ms', 'energy',\n# 'instrumentalness', 'liveness', 'loudness', 'speechiness',\n# 'tempo', 'valence', 'popularity'\nsns.lmplot(x='energy', y='duration_ms', data=clusterdata, hue='labels', fit_reg=False, height = 6, aspect = 2)\nsns.lmplot(x='popularity', y='duration_ms', data=clusterdata, hue='labels', fit_reg=False, height = 6, aspect = 2)\nsns.lmplot(x='energy', y='loudness', data=clusterdata, hue='labels', fit_reg=False, height = 6, aspect = 2)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T21:02:56.998766Z","iopub.execute_input":"2021-08-27T21:02:56.999375Z","iopub.status.idle":"2021-08-27T21:03:02.353184Z","shell.execute_reply.started":"2021-08-27T21:02:56.999326Z","shell.execute_reply":"2021-08-27T21:03:02.352057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When examining our optimal cluster fit, there is a noticeable cluster divide on many 2-D slices involving the duration_ms column. This could possibly hint that this data feature is an important one in splitting the data, and could play a role in giving good recommendations. \n\nHowever, seeing as some of the columns were more correlated than others in our heatmap, such as loudness, energy, and tempo, it could be that clusters do not emerge between other features that'd probably play a bigger role in determining whether a song's fit for recommendation or not because of their overlap. Moving forward, PCA could be a good idea in assisting in assigning better clusters, but with any dimensionality reduction technique, some interpretability is sacrificed as a trade off.\n\nAnother good idea is to see whether individual columns tends to cluster as dramatically as duration_ms does. For example, are songs of a certain loudness, danceability, etc., clustered together?","metadata":{}},{"cell_type":"markdown","source":"**RECOMMENDER SYSTEM**\n\nFrom here we start building our recommender system. Since there are no users within this dataset, this system will rely strictly on similarity between any two song's characteristics. We investigated these earlier utilizing a some basic data exploratory, a correlation heatmap, and cluster analysis.\n\nAs such, the content-based filtering approach based on feature similarity will work best for us, which we will compose as a class and package all of its features and functions within this framework.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nclass SpotifyRecommender(object):\n    \n    def __init__(self, max_num = 5):\n        if max_num > 30:\n            print('No larger than 30 similar songs should be recommended at one time; value set to thirty')\n            max_num = 30\n        elif max_num <= 0:\n            print('Have to recommend at least one song... value set to 5')\n            max_num = 5\n        self.max_num_recommendations = max_num\n        self.recommendations = {}\n    \n    def train(self, data):\n        '''\n        Trains our recommender system using the data provided in the function call\n        \n        We take in a data matrix of all the song data we'd wish to utilize when training,\n        create an index of each song's most similar songs within our dataset based on scikit-learn's\n        cosine similarity metric, and then store this for each song id.\n        \n        RETURN: Nothing\n        '''\n        # maybe use linear_kernel?\n        self.data = data\n        \n        # compute a similarity matrix for all of the data points to all other data points\n        similarity = cosine_similarity(self.data[['acousticness', 'danceability', 'duration_ms', 'energy',\n                         'instrumentalness', 'liveness', 'loudness', 'speechiness',\n                         'tempo', 'valence', 'popularity']])\n        \n        # iterate through the rows of data\n        for idx, row in self.data.iterrows():\n            \n            # get the most similar indices to the current row of our similarity matrix, \n            # remove first entry because it's most similar\n            similar_indices = similarity[idx].argsort()[:-1*(self.max_num_recommendations+1):-1]\n            # debug statement\n            # print(len(similar_indices))\n            \n            # index into data frame and append a list of (cosine similarity score, similar track id) pairs\n            # to the song with track_id row[id] in our dictionary\n            similar_tracks = [(similarity[idx][i], data['id'][i]) \n                              for i in similar_indices]\n            \n            # append recommendations to dictionary by song id\n            self.recommendations[row['id']] = similar_tracks[1:]\n            \n    \n    def retrieve(self, track_id = None):\n        '''\n        This function does all of the retrieval work of a song's more involved details so that\n        recommendations can be done simply by an id being passed\n        \n        RETURN: Artist and Name contained within row of provided dataframe indexed at track_id\n        '''\n        temp = self.data.loc[self.data[\"id\"] == track_id][['artists', 'name']]\n        artists = temp['artists']\n        name = temp['name']\n        return [artists, name]\n    \n    \n    def predict(self, track_id = None):\n        '''\n        Simply put... just \"predict\" (i.e. return) the most similar songs\n        \n        Given a track_id, output the (max_num_recommendations) most similar songs in our dataset\n        \n        RETURN: The (max_num_recommendations) most similar tracks for given (track_id)\n        '''\n        # get song's data\n        song = self.retrieve(track_id)\n        \n        # print\n        print(\"Retrieving song recommendations similar to \" + song[1] + \" by \" + song[0] + '...')\n        \n        # for entry brought up in our recommendations table\n        for recommended_song in self.recommendations[track_id]:\n            print(self.retrieve(recommended_song[1])[0])\n            print(self.retrieve(recommended_song[1])[1])\n            print(\"Recommended song: ID: \" + str(recommended_song[1]) + \" Name: \" + self.retrieve(recommended_song[1])[1] \n                  + \" by Artist: \" + self.retrieve(recommended_song[1])[0] +  \" . Similarity score: \" + str(recommended_song[0]))\n        ","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:19:15.226678Z","iopub.execute_input":"2021-08-20T17:19:15.22718Z","iopub.status.idle":"2021-08-20T17:19:15.241278Z","shell.execute_reply.started":"2021-08-20T17:19:15.227148Z","shell.execute_reply":"2021-08-20T17:19:15.240588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommender = SpotifyRecommender(5)\nrecommender.train(tracks.iloc[:100])\nrecommender.predict(tracks.iloc[0]['id'])","metadata":{"execution":{"iopub.status.busy":"2021-08-20T17:19:18.0636Z","iopub.execute_input":"2021-08-20T17:19:18.064195Z","iopub.status.idle":"2021-08-20T17:19:18.145385Z","shell.execute_reply.started":"2021-08-20T17:19:18.064159Z","shell.execute_reply":"2021-08-20T17:19:18.144328Z"},"trusted":true},"execution_count":null,"outputs":[]}]}